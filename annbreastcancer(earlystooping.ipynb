{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEgIsoAr0Pri"
      },
      "outputs": [],
      "source": [
        "# IMPORTING LIBRARIES\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings \n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.metrics import classification_report\n"
      ],
      "metadata": {
        "id": "BTOsKijZ0ofX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#READING THE EXCEL FILE\n",
        "df=pd.read_csv(\"breast-cancer.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "hIehMFnl1d_x",
        "outputId": "8a66968b-6b89-4fef-a743-bfe370bf0b7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
              "0    842302         M        17.99         10.38          122.80     1001.0   \n",
              "1    842517         M        20.57         17.77          132.90     1326.0   \n",
              "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
              "3  84348301         M        11.42         20.38           77.58      386.1   \n",
              "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
              "\n",
              "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
              "0          0.11840           0.27760          0.3001              0.14710   \n",
              "1          0.08474           0.07864          0.0869              0.07017   \n",
              "2          0.10960           0.15990          0.1974              0.12790   \n",
              "3          0.14250           0.28390          0.2414              0.10520   \n",
              "4          0.10030           0.13280          0.1980              0.10430   \n",
              "\n",
              "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
              "0  ...         25.38          17.33           184.60      2019.0   \n",
              "1  ...         24.99          23.41           158.80      1956.0   \n",
              "2  ...         23.57          25.53           152.50      1709.0   \n",
              "3  ...         14.91          26.50            98.87       567.7   \n",
              "4  ...         22.54          16.67           152.20      1575.0   \n",
              "\n",
              "   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n",
              "0            0.1622             0.6656           0.7119                0.2654   \n",
              "1            0.1238             0.1866           0.2416                0.1860   \n",
              "2            0.1444             0.4245           0.4504                0.2430   \n",
              "3            0.2098             0.8663           0.6869                0.2575   \n",
              "4            0.1374             0.2050           0.4000                0.1625   \n",
              "\n",
              "   symmetry_worst  fractal_dimension_worst  \n",
              "0          0.4601                  0.11890  \n",
              "1          0.2750                  0.08902  \n",
              "2          0.3613                  0.08758  \n",
              "3          0.6638                  0.17300  \n",
              "4          0.2364                  0.07678  \n",
              "\n",
              "[5 rows x 32 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a659add1-bee9-4040-b856-5b9eba15cf80\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>...</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>...</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>...</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>...</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>...</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>...</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 32 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a659add1-bee9-4040-b856-5b9eba15cf80')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a659add1-bee9-4040-b856-5b9eba15cf80 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a659add1-bee9-4040-b856-5b9eba15cf80');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#HANDLING CATEGORICAL VALUES\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le=LabelEncoder()"
      ],
      "metadata": {
        "id": "UwZ_TnwN10T0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"diagnosis\"]=le.fit_transform(df[\"diagnosis\"])\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "PV4NDkgP3dpr",
        "outputId": "fa355c97-f056-4eee-8597-4dd1b6ee1cb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id  diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
              "0    842302          1        17.99         10.38          122.80     1001.0   \n",
              "1    842517          1        20.57         17.77          132.90     1326.0   \n",
              "2  84300903          1        19.69         21.25          130.00     1203.0   \n",
              "3  84348301          1        11.42         20.38           77.58      386.1   \n",
              "4  84358402          1        20.29         14.34          135.10     1297.0   \n",
              "\n",
              "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
              "0          0.11840           0.27760          0.3001              0.14710   \n",
              "1          0.08474           0.07864          0.0869              0.07017   \n",
              "2          0.10960           0.15990          0.1974              0.12790   \n",
              "3          0.14250           0.28390          0.2414              0.10520   \n",
              "4          0.10030           0.13280          0.1980              0.10430   \n",
              "\n",
              "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
              "0  ...         25.38          17.33           184.60      2019.0   \n",
              "1  ...         24.99          23.41           158.80      1956.0   \n",
              "2  ...         23.57          25.53           152.50      1709.0   \n",
              "3  ...         14.91          26.50            98.87       567.7   \n",
              "4  ...         22.54          16.67           152.20      1575.0   \n",
              "\n",
              "   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n",
              "0            0.1622             0.6656           0.7119                0.2654   \n",
              "1            0.1238             0.1866           0.2416                0.1860   \n",
              "2            0.1444             0.4245           0.4504                0.2430   \n",
              "3            0.2098             0.8663           0.6869                0.2575   \n",
              "4            0.1374             0.2050           0.4000                0.1625   \n",
              "\n",
              "   symmetry_worst  fractal_dimension_worst  \n",
              "0          0.4601                  0.11890  \n",
              "1          0.2750                  0.08902  \n",
              "2          0.3613                  0.08758  \n",
              "3          0.6638                  0.17300  \n",
              "4          0.2364                  0.07678  \n",
              "\n",
              "[5 rows x 32 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2a0a8eaa-cb95-446e-b5af-a1ff0b8097b8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>...</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>1</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>...</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>1</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>...</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>1</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>...</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>1</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>...</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>1</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>...</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 32 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2a0a8eaa-cb95-446e-b5af-a1ff0b8097b8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2a0a8eaa-cb95-446e-b5af-a1ff0b8097b8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2a0a8eaa-cb95-446e-b5af-a1ff0b8097b8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SEAPRATING X AND Y i.e. FEATURE AND TARGET\n",
        "\n",
        "x=df.iloc[:,2:].values\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvssnnX630Ms",
        "outputId": "224b32bf-459e-4ad5-e1d9-e9681f9f64da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
              "        1.189e-01],\n",
              "       [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
              "        8.902e-02],\n",
              "       [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
              "        8.758e-02],\n",
              "       ...,\n",
              "       [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
              "        7.820e-02],\n",
              "       [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
              "        1.240e-01],\n",
              "       [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
              "        7.039e-02]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y=df.iloc[:,1].values\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bm9DRjGa43Xz",
        "outputId": "aae1f505-0826-45d1-ab3a-c5a14a375d08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
              "       0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
              "       0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
              "       1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n",
              "       1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
              "       1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SPLITTING INTO XTRAIN AND YTRAIN\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "xtrain,xtest,ytrain,ytest=train_test_split(x,y,random_state=1,test_size=0.20)"
      ],
      "metadata": {
        "id": "oF6DcnOX5QHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SCALING IN PROPER MANNER\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "ss=StandardScaler()\n",
        "xtrain=ss.fit_transform(xtrain)\n",
        "xtest=ss.transform(xtest)"
      ],
      "metadata": {
        "id": "rRYBfyo_5z_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#APPLYING EARLY STOPPING \n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stop=EarlyStopping(monitor=\"val_loss\",mode=\"min\",verbose=1,patience=9)"
      ],
      "metadata": {
        "id": "JvjGQkbqKnTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ADDING HIDDEN LAYERS AND OUTPUT LAYERS \n",
        "ann=Sequential()\n",
        "\n",
        "ann.add(Dense(units=8,activation=\"relu\"))\n",
        "ann.add(Dense(units=9,activation=\"relu\"))\n",
        "ann.add(Dense(units=6,activation=\"relu\"))\n",
        "ann.add(Dense(units=1,activation=\"sigmoid\"))"
      ],
      "metadata": {
        "id": "PIqnTTfg6c_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ann.compile(optimizer=\"sgd\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
        "\n",
        "ann.fit(xtrain,ytrain,validation_data=(xtest,ytest),verbose=1,callbacks=[early_stop],batch_size=40,epochs=600)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9Xah1Mo7K2L",
        "outputId": "40fa6c99-ea72-432e-d04b-41dba85c73c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/600\n",
            "12/12 [==============================] - 1s 22ms/step - loss: 0.6737 - accuracy: 0.6088 - val_loss: 0.6714 - val_accuracy: 0.6053\n",
            "Epoch 2/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6445 - accuracy: 0.6418 - val_loss: 0.6471 - val_accuracy: 0.6404\n",
            "Epoch 3/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6199 - accuracy: 0.6703 - val_loss: 0.6250 - val_accuracy: 0.6754\n",
            "Epoch 4/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.5986 - accuracy: 0.7033 - val_loss: 0.6048 - val_accuracy: 0.6842\n",
            "Epoch 5/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.5781 - accuracy: 0.7341 - val_loss: 0.5850 - val_accuracy: 0.7105\n",
            "Epoch 6/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.5575 - accuracy: 0.7604 - val_loss: 0.5663 - val_accuracy: 0.7281\n",
            "Epoch 7/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.5375 - accuracy: 0.7736 - val_loss: 0.5481 - val_accuracy: 0.7632\n",
            "Epoch 8/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.5179 - accuracy: 0.8066 - val_loss: 0.5305 - val_accuracy: 0.7719\n",
            "Epoch 9/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.4987 - accuracy: 0.8374 - val_loss: 0.5127 - val_accuracy: 0.7807\n",
            "Epoch 10/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.4797 - accuracy: 0.8549 - val_loss: 0.4957 - val_accuracy: 0.7895\n",
            "Epoch 11/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.4614 - accuracy: 0.8681 - val_loss: 0.4800 - val_accuracy: 0.8070\n",
            "Epoch 12/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.4437 - accuracy: 0.8857 - val_loss: 0.4646 - val_accuracy: 0.8158\n",
            "Epoch 13/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.4264 - accuracy: 0.8901 - val_loss: 0.4490 - val_accuracy: 0.8246\n",
            "Epoch 14/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.4098 - accuracy: 0.9055 - val_loss: 0.4349 - val_accuracy: 0.8246\n",
            "Epoch 15/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.3933 - accuracy: 0.9055 - val_loss: 0.4213 - val_accuracy: 0.8509\n",
            "Epoch 16/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.3772 - accuracy: 0.9077 - val_loss: 0.4076 - val_accuracy: 0.8684\n",
            "Epoch 17/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.3609 - accuracy: 0.9077 - val_loss: 0.3936 - val_accuracy: 0.8947\n",
            "Epoch 18/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.3440 - accuracy: 0.9231 - val_loss: 0.3788 - val_accuracy: 0.8947\n",
            "Epoch 19/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.3264 - accuracy: 0.9253 - val_loss: 0.3659 - val_accuracy: 0.8947\n",
            "Epoch 20/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.3096 - accuracy: 0.9253 - val_loss: 0.3545 - val_accuracy: 0.8947\n",
            "Epoch 21/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2944 - accuracy: 0.9253 - val_loss: 0.3434 - val_accuracy: 0.8947\n",
            "Epoch 22/600\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.2796 - accuracy: 0.9297 - val_loss: 0.3323 - val_accuracy: 0.9035\n",
            "Epoch 23/600\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.2656 - accuracy: 0.9297 - val_loss: 0.3214 - val_accuracy: 0.9035\n",
            "Epoch 24/600\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.2527 - accuracy: 0.9319 - val_loss: 0.3123 - val_accuracy: 0.9123\n",
            "Epoch 25/600\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.2411 - accuracy: 0.9341 - val_loss: 0.3034 - val_accuracy: 0.9123\n",
            "Epoch 26/600\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.2306 - accuracy: 0.9385 - val_loss: 0.2947 - val_accuracy: 0.9035\n",
            "Epoch 27/600\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.2212 - accuracy: 0.9407 - val_loss: 0.2873 - val_accuracy: 0.9035\n",
            "Epoch 28/600\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.2123 - accuracy: 0.9429 - val_loss: 0.2799 - val_accuracy: 0.9123\n",
            "Epoch 29/600\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.2044 - accuracy: 0.9473 - val_loss: 0.2740 - val_accuracy: 0.9123\n",
            "Epoch 30/600\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.1971 - accuracy: 0.9473 - val_loss: 0.2681 - val_accuracy: 0.9123\n",
            "Epoch 31/600\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1903 - accuracy: 0.9495 - val_loss: 0.2619 - val_accuracy: 0.9123\n",
            "Epoch 32/600\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1839 - accuracy: 0.9495 - val_loss: 0.2567 - val_accuracy: 0.9123\n",
            "Epoch 33/600\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.1778 - accuracy: 0.9516 - val_loss: 0.2516 - val_accuracy: 0.9211\n",
            "Epoch 34/600\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.1721 - accuracy: 0.9516 - val_loss: 0.2464 - val_accuracy: 0.9211\n",
            "Epoch 35/600\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.1667 - accuracy: 0.9560 - val_loss: 0.2412 - val_accuracy: 0.9211\n",
            "Epoch 36/600\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.1619 - accuracy: 0.9516 - val_loss: 0.2361 - val_accuracy: 0.9211\n",
            "Epoch 37/600\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.1570 - accuracy: 0.9538 - val_loss: 0.2306 - val_accuracy: 0.9211\n",
            "Epoch 38/600\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1527 - accuracy: 0.9560 - val_loss: 0.2266 - val_accuracy: 0.9211\n",
            "Epoch 39/600\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.1486 - accuracy: 0.9560 - val_loss: 0.2220 - val_accuracy: 0.9298\n",
            "Epoch 40/600\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.1447 - accuracy: 0.9560 - val_loss: 0.2173 - val_accuracy: 0.9298\n",
            "Epoch 41/600\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.1411 - accuracy: 0.9538 - val_loss: 0.2130 - val_accuracy: 0.9386\n",
            "Epoch 42/600\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1379 - accuracy: 0.9560 - val_loss: 0.2088 - val_accuracy: 0.9386\n",
            "Epoch 43/600\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.1349 - accuracy: 0.9560 - val_loss: 0.2051 - val_accuracy: 0.9386\n",
            "Epoch 44/600\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.1319 - accuracy: 0.9560 - val_loss: 0.2022 - val_accuracy: 0.9386\n",
            "Epoch 45/600\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.1290 - accuracy: 0.9582 - val_loss: 0.1992 - val_accuracy: 0.9386\n",
            "Epoch 46/600\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1265 - accuracy: 0.9582 - val_loss: 0.1965 - val_accuracy: 0.9474\n",
            "Epoch 47/600\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.1241 - accuracy: 0.9626 - val_loss: 0.1930 - val_accuracy: 0.9474\n",
            "Epoch 48/600\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.1216 - accuracy: 0.9626 - val_loss: 0.1899 - val_accuracy: 0.9474\n",
            "Epoch 49/600\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1192 - accuracy: 0.9626 - val_loss: 0.1867 - val_accuracy: 0.9474\n",
            "Epoch 50/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1170 - accuracy: 0.9604 - val_loss: 0.1841 - val_accuracy: 0.9474\n",
            "Epoch 51/600\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.1150 - accuracy: 0.9604 - val_loss: 0.1819 - val_accuracy: 0.9474\n",
            "Epoch 52/600\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.1128 - accuracy: 0.9604 - val_loss: 0.1796 - val_accuracy: 0.9474\n",
            "Epoch 53/600\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.1109 - accuracy: 0.9604 - val_loss: 0.1777 - val_accuracy: 0.9474\n",
            "Epoch 54/600\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.1089 - accuracy: 0.9604 - val_loss: 0.1744 - val_accuracy: 0.9561\n",
            "Epoch 55/600\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.1070 - accuracy: 0.9604 - val_loss: 0.1717 - val_accuracy: 0.9561\n",
            "Epoch 56/600\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1053 - accuracy: 0.9604 - val_loss: 0.1698 - val_accuracy: 0.9561\n",
            "Epoch 57/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1035 - accuracy: 0.9626 - val_loss: 0.1681 - val_accuracy: 0.9561\n",
            "Epoch 58/600\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1019 - accuracy: 0.9626 - val_loss: 0.1666 - val_accuracy: 0.9561\n",
            "Epoch 59/600\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.1003 - accuracy: 0.9626 - val_loss: 0.1641 - val_accuracy: 0.9561\n",
            "Epoch 60/600\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.0988 - accuracy: 0.9648 - val_loss: 0.1625 - val_accuracy: 0.9561\n",
            "Epoch 61/600\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.0972 - accuracy: 0.9692 - val_loss: 0.1611 - val_accuracy: 0.9561\n",
            "Epoch 62/600\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.0957 - accuracy: 0.9670 - val_loss: 0.1594 - val_accuracy: 0.9561\n",
            "Epoch 63/600\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.0943 - accuracy: 0.9692 - val_loss: 0.1580 - val_accuracy: 0.9561\n",
            "Epoch 64/600\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.0929 - accuracy: 0.9714 - val_loss: 0.1567 - val_accuracy: 0.9561\n",
            "Epoch 65/600\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.0915 - accuracy: 0.9714 - val_loss: 0.1555 - val_accuracy: 0.9561\n",
            "Epoch 66/600\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.0903 - accuracy: 0.9714 - val_loss: 0.1545 - val_accuracy: 0.9649\n",
            "Epoch 67/600\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.0891 - accuracy: 0.9714 - val_loss: 0.1532 - val_accuracy: 0.9649\n",
            "Epoch 68/600\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.0879 - accuracy: 0.9714 - val_loss: 0.1518 - val_accuracy: 0.9649\n",
            "Epoch 69/600\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0868 - accuracy: 0.9714 - val_loss: 0.1506 - val_accuracy: 0.9649\n",
            "Epoch 70/600\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.0857 - accuracy: 0.9714 - val_loss: 0.1496 - val_accuracy: 0.9649\n",
            "Epoch 71/600\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0847 - accuracy: 0.9714 - val_loss: 0.1478 - val_accuracy: 0.9649\n",
            "Epoch 72/600\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.0836 - accuracy: 0.9714 - val_loss: 0.1468 - val_accuracy: 0.9649\n",
            "Epoch 73/600\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.0827 - accuracy: 0.9736 - val_loss: 0.1457 - val_accuracy: 0.9649\n",
            "Epoch 74/600\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.0818 - accuracy: 0.9736 - val_loss: 0.1451 - val_accuracy: 0.9649\n",
            "Epoch 75/600\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.0808 - accuracy: 0.9736 - val_loss: 0.1444 - val_accuracy: 0.9649\n",
            "Epoch 76/600\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.0800 - accuracy: 0.9758 - val_loss: 0.1440 - val_accuracy: 0.9649\n",
            "Epoch 77/600\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.0790 - accuracy: 0.9758 - val_loss: 0.1432 - val_accuracy: 0.9649\n",
            "Epoch 78/600\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 0.0780 - accuracy: 0.9758 - val_loss: 0.1424 - val_accuracy: 0.9649\n",
            "Epoch 79/600\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.0764 - accuracy: 0.9758 - val_loss: 0.1418 - val_accuracy: 0.9649\n",
            "Epoch 80/600\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.0753 - accuracy: 0.9758 - val_loss: 0.1408 - val_accuracy: 0.9649\n",
            "Epoch 81/600\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.0741 - accuracy: 0.9758 - val_loss: 0.1401 - val_accuracy: 0.9649\n",
            "Epoch 82/600\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 0.0731 - accuracy: 0.9780 - val_loss: 0.1393 - val_accuracy: 0.9649\n",
            "Epoch 83/600\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.0721 - accuracy: 0.9780 - val_loss: 0.1389 - val_accuracy: 0.9649\n",
            "Epoch 84/600\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.0711 - accuracy: 0.9802 - val_loss: 0.1387 - val_accuracy: 0.9649\n",
            "Epoch 85/600\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.0703 - accuracy: 0.9780 - val_loss: 0.1382 - val_accuracy: 0.9649\n",
            "Epoch 86/600\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0694 - accuracy: 0.9780 - val_loss: 0.1375 - val_accuracy: 0.9649\n",
            "Epoch 87/600\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.0686 - accuracy: 0.9780 - val_loss: 0.1371 - val_accuracy: 0.9649\n",
            "Epoch 88/600\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 0.0678 - accuracy: 0.9780 - val_loss: 0.1364 - val_accuracy: 0.9649\n",
            "Epoch 89/600\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.0671 - accuracy: 0.9802 - val_loss: 0.1361 - val_accuracy: 0.9649\n",
            "Epoch 90/600\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.0663 - accuracy: 0.9802 - val_loss: 0.1356 - val_accuracy: 0.9649\n",
            "Epoch 91/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0656 - accuracy: 0.9802 - val_loss: 0.1350 - val_accuracy: 0.9649\n",
            "Epoch 92/600\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.0650 - accuracy: 0.9802 - val_loss: 0.1345 - val_accuracy: 0.9649\n",
            "Epoch 93/600\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.0644 - accuracy: 0.9802 - val_loss: 0.1336 - val_accuracy: 0.9649\n",
            "Epoch 94/600\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.0639 - accuracy: 0.9802 - val_loss: 0.1326 - val_accuracy: 0.9649\n",
            "Epoch 95/600\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0634 - accuracy: 0.9802 - val_loss: 0.1323 - val_accuracy: 0.9649\n",
            "Epoch 96/600\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.0629 - accuracy: 0.9780 - val_loss: 0.1321 - val_accuracy: 0.9649\n",
            "Epoch 97/600\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.0623 - accuracy: 0.9780 - val_loss: 0.1319 - val_accuracy: 0.9649\n",
            "Epoch 98/600\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0619 - accuracy: 0.9780 - val_loss: 0.1314 - val_accuracy: 0.9649\n",
            "Epoch 99/600\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0613 - accuracy: 0.9780 - val_loss: 0.1310 - val_accuracy: 0.9649\n",
            "Epoch 100/600\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0609 - accuracy: 0.9780 - val_loss: 0.1307 - val_accuracy: 0.9649\n",
            "Epoch 101/600\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0604 - accuracy: 0.9780 - val_loss: 0.1305 - val_accuracy: 0.9649\n",
            "Epoch 102/600\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0599 - accuracy: 0.9780 - val_loss: 0.1303 - val_accuracy: 0.9649\n",
            "Epoch 103/600\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.0596 - accuracy: 0.9802 - val_loss: 0.1301 - val_accuracy: 0.9649\n",
            "Epoch 104/600\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0591 - accuracy: 0.9802 - val_loss: 0.1298 - val_accuracy: 0.9649\n",
            "Epoch 105/600\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.0588 - accuracy: 0.9802 - val_loss: 0.1296 - val_accuracy: 0.9649\n",
            "Epoch 106/600\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0583 - accuracy: 0.9802 - val_loss: 0.1293 - val_accuracy: 0.9649\n",
            "Epoch 107/600\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0579 - accuracy: 0.9802 - val_loss: 0.1292 - val_accuracy: 0.9649\n",
            "Epoch 108/600\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0576 - accuracy: 0.9824 - val_loss: 0.1290 - val_accuracy: 0.9649\n",
            "Epoch 109/600\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.0573 - accuracy: 0.9824 - val_loss: 0.1288 - val_accuracy: 0.9649\n",
            "Epoch 110/600\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0568 - accuracy: 0.9824 - val_loss: 0.1285 - val_accuracy: 0.9649\n",
            "Epoch 111/600\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0565 - accuracy: 0.9824 - val_loss: 0.1286 - val_accuracy: 0.9649\n",
            "Epoch 112/600\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.0560 - accuracy: 0.9846 - val_loss: 0.1283 - val_accuracy: 0.9649\n",
            "Epoch 113/600\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0557 - accuracy: 0.9868 - val_loss: 0.1274 - val_accuracy: 0.9649\n",
            "Epoch 114/600\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.0554 - accuracy: 0.9824 - val_loss: 0.1272 - val_accuracy: 0.9649\n",
            "Epoch 115/600\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.0550 - accuracy: 0.9824 - val_loss: 0.1271 - val_accuracy: 0.9649\n",
            "Epoch 116/600\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.0546 - accuracy: 0.9846 - val_loss: 0.1268 - val_accuracy: 0.9649\n",
            "Epoch 117/600\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.0543 - accuracy: 0.9824 - val_loss: 0.1268 - val_accuracy: 0.9649\n",
            "Epoch 118/600\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0540 - accuracy: 0.9824 - val_loss: 0.1265 - val_accuracy: 0.9649\n",
            "Epoch 119/600\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0537 - accuracy: 0.9846 - val_loss: 0.1263 - val_accuracy: 0.9649\n",
            "Epoch 120/600\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.0533 - accuracy: 0.9846 - val_loss: 0.1260 - val_accuracy: 0.9649\n",
            "Epoch 121/600\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.0530 - accuracy: 0.9824 - val_loss: 0.1253 - val_accuracy: 0.9649\n",
            "Epoch 122/600\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.0527 - accuracy: 0.9824 - val_loss: 0.1251 - val_accuracy: 0.9649\n",
            "Epoch 123/600\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0524 - accuracy: 0.9846 - val_loss: 0.1251 - val_accuracy: 0.9649\n",
            "Epoch 124/600\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.0520 - accuracy: 0.9868 - val_loss: 0.1249 - val_accuracy: 0.9649\n",
            "Epoch 125/600\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.0516 - accuracy: 0.9868 - val_loss: 0.1247 - val_accuracy: 0.9649\n",
            "Epoch 126/600\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.0513 - accuracy: 0.9846 - val_loss: 0.1248 - val_accuracy: 0.9649\n",
            "Epoch 127/600\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.0511 - accuracy: 0.9846 - val_loss: 0.1247 - val_accuracy: 0.9649\n",
            "Epoch 128/600\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.0507 - accuracy: 0.9868 - val_loss: 0.1247 - val_accuracy: 0.9649\n",
            "Epoch 129/600\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.0504 - accuracy: 0.9868 - val_loss: 0.1245 - val_accuracy: 0.9649\n",
            "Epoch 130/600\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0501 - accuracy: 0.9868 - val_loss: 0.1244 - val_accuracy: 0.9649\n",
            "Epoch 131/600\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0498 - accuracy: 0.9868 - val_loss: 0.1243 - val_accuracy: 0.9649\n",
            "Epoch 132/600\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.0496 - accuracy: 0.9868 - val_loss: 0.1243 - val_accuracy: 0.9649\n",
            "Epoch 133/600\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.0493 - accuracy: 0.9868 - val_loss: 0.1242 - val_accuracy: 0.9649\n",
            "Epoch 134/600\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0489 - accuracy: 0.9868 - val_loss: 0.1241 - val_accuracy: 0.9649\n",
            "Epoch 135/600\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0487 - accuracy: 0.9868 - val_loss: 0.1239 - val_accuracy: 0.9649\n",
            "Epoch 136/600\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 0.0484 - accuracy: 0.9868 - val_loss: 0.1237 - val_accuracy: 0.9649\n",
            "Epoch 137/600\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.0482 - accuracy: 0.9868 - val_loss: 0.1238 - val_accuracy: 0.9649\n",
            "Epoch 138/600\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.0479 - accuracy: 0.9868 - val_loss: 0.1237 - val_accuracy: 0.9649\n",
            "Epoch 139/600\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.0476 - accuracy: 0.9868 - val_loss: 0.1235 - val_accuracy: 0.9649\n",
            "Epoch 140/600\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.0474 - accuracy: 0.9890 - val_loss: 0.1234 - val_accuracy: 0.9649\n",
            "Epoch 141/600\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.0471 - accuracy: 0.9890 - val_loss: 0.1236 - val_accuracy: 0.9649\n",
            "Epoch 142/600\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0469 - accuracy: 0.9890 - val_loss: 0.1228 - val_accuracy: 0.9649\n",
            "Epoch 143/600\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.0466 - accuracy: 0.9890 - val_loss: 0.1226 - val_accuracy: 0.9649\n",
            "Epoch 144/600\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.0464 - accuracy: 0.9890 - val_loss: 0.1218 - val_accuracy: 0.9649\n",
            "Epoch 145/600\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.0461 - accuracy: 0.9890 - val_loss: 0.1218 - val_accuracy: 0.9649\n",
            "Epoch 146/600\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.0458 - accuracy: 0.9890 - val_loss: 0.1219 - val_accuracy: 0.9649\n",
            "Epoch 147/600\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 0.0456 - accuracy: 0.9890 - val_loss: 0.1218 - val_accuracy: 0.9649\n",
            "Epoch 148/600\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 0.0455 - accuracy: 0.9890 - val_loss: 0.1218 - val_accuracy: 0.9649\n",
            "Epoch 149/600\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.0452 - accuracy: 0.9890 - val_loss: 0.1218 - val_accuracy: 0.9649\n",
            "Epoch 150/600\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.0450 - accuracy: 0.9890 - val_loss: 0.1210 - val_accuracy: 0.9649\n",
            "Epoch 151/600\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0449 - accuracy: 0.9890 - val_loss: 0.1208 - val_accuracy: 0.9649\n",
            "Epoch 152/600\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.0446 - accuracy: 0.9890 - val_loss: 0.1209 - val_accuracy: 0.9649\n",
            "Epoch 153/600\n",
            "12/12 [==============================] - 0s 32ms/step - loss: 0.0444 - accuracy: 0.9890 - val_loss: 0.1210 - val_accuracy: 0.9649\n",
            "Epoch 154/600\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.0442 - accuracy: 0.9890 - val_loss: 0.1209 - val_accuracy: 0.9649\n",
            "Epoch 155/600\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.0440 - accuracy: 0.9890 - val_loss: 0.1212 - val_accuracy: 0.9649\n",
            "Epoch 156/600\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.0438 - accuracy: 0.9890 - val_loss: 0.1211 - val_accuracy: 0.9649\n",
            "Epoch 157/600\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.0437 - accuracy: 0.9890 - val_loss: 0.1215 - val_accuracy: 0.9649\n",
            "Epoch 158/600\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 0.0435 - accuracy: 0.9890 - val_loss: 0.1216 - val_accuracy: 0.9649\n",
            "Epoch 159/600\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.0434 - accuracy: 0.9890 - val_loss: 0.1215 - val_accuracy: 0.9649\n",
            "Epoch 160/600\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.0432 - accuracy: 0.9890 - val_loss: 0.1220 - val_accuracy: 0.9649\n",
            "Epoch 160: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f93da62cd00>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ann.history.history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hm_U_0XKNErm",
        "outputId": "c53e13b5-9cc2-4c29-ef80-f2edaa595c3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [0.6736800074577332,\n",
              "  0.6444770097732544,\n",
              "  0.6199088096618652,\n",
              "  0.598555326461792,\n",
              "  0.5781198143959045,\n",
              "  0.5575145483016968,\n",
              "  0.5375059247016907,\n",
              "  0.5179463624954224,\n",
              "  0.49867039918899536,\n",
              "  0.47973695397377014,\n",
              "  0.4613807201385498,\n",
              "  0.4436730146408081,\n",
              "  0.4263646602630615,\n",
              "  0.4097830355167389,\n",
              "  0.39330628514289856,\n",
              "  0.37717461585998535,\n",
              "  0.3608686625957489,\n",
              "  0.34396955370903015,\n",
              "  0.32635146379470825,\n",
              "  0.3095616400241852,\n",
              "  0.294422447681427,\n",
              "  0.2795521020889282,\n",
              "  0.26561981439590454,\n",
              "  0.25274667143821716,\n",
              "  0.24113768339157104,\n",
              "  0.23062175512313843,\n",
              "  0.22122006118297577,\n",
              "  0.21229179203510284,\n",
              "  0.20438475906848907,\n",
              "  0.19713690876960754,\n",
              "  0.1902725249528885,\n",
              "  0.1839284896850586,\n",
              "  0.17784874141216278,\n",
              "  0.17208555340766907,\n",
              "  0.1666969209909439,\n",
              "  0.16191403567790985,\n",
              "  0.15704558789730072,\n",
              "  0.15268318355083466,\n",
              "  0.14862659573554993,\n",
              "  0.14470629394054413,\n",
              "  0.1410924792289734,\n",
              "  0.13785578310489655,\n",
              "  0.13486774265766144,\n",
              "  0.13190248608589172,\n",
              "  0.12903660535812378,\n",
              "  0.12653233110904694,\n",
              "  0.12412428110837936,\n",
              "  0.12157046794891357,\n",
              "  0.11920765787363052,\n",
              "  0.11703440546989441,\n",
              "  0.1150343269109726,\n",
              "  0.11283151060342789,\n",
              "  0.11088827252388,\n",
              "  0.10889149457216263,\n",
              "  0.10704702138900757,\n",
              "  0.10526888817548752,\n",
              "  0.10352953523397446,\n",
              "  0.10193265974521637,\n",
              "  0.10032398253679276,\n",
              "  0.0987938866019249,\n",
              "  0.09716442972421646,\n",
              "  0.09567014873027802,\n",
              "  0.094317726790905,\n",
              "  0.0928778201341629,\n",
              "  0.09148663282394409,\n",
              "  0.09028386324644089,\n",
              "  0.08906801789999008,\n",
              "  0.08786365389823914,\n",
              "  0.08678456395864487,\n",
              "  0.08569199591875076,\n",
              "  0.08467389643192291,\n",
              "  0.08362209051847458,\n",
              "  0.08269321173429489,\n",
              "  0.0817883238196373,\n",
              "  0.08083666115999222,\n",
              "  0.07995632290840149,\n",
              "  0.07900140434503555,\n",
              "  0.07798037678003311,\n",
              "  0.07635878771543503,\n",
              "  0.07529334723949432,\n",
              "  0.07410174608230591,\n",
              "  0.07312054187059402,\n",
              "  0.0721328929066658,\n",
              "  0.07109349220991135,\n",
              "  0.07025966793298721,\n",
              "  0.06939497590065002,\n",
              "  0.06858459115028381,\n",
              "  0.06782449781894684,\n",
              "  0.06706792861223221,\n",
              "  0.06631618738174438,\n",
              "  0.06562944501638412,\n",
              "  0.06496872752904892,\n",
              "  0.0644296184182167,\n",
              "  0.06387676298618317,\n",
              "  0.06342922896146774,\n",
              "  0.06294146180152893,\n",
              "  0.0623326301574707,\n",
              "  0.06189783290028572,\n",
              "  0.06131119653582573,\n",
              "  0.06091389060020447,\n",
              "  0.06037706881761551,\n",
              "  0.05993467941880226,\n",
              "  0.05960724130272865,\n",
              "  0.05914781242609024,\n",
              "  0.058761972934007645,\n",
              "  0.05830870941281319,\n",
              "  0.05793774127960205,\n",
              "  0.05758261680603027,\n",
              "  0.05726051330566406,\n",
              "  0.056796859949827194,\n",
              "  0.05649302154779434,\n",
              "  0.056035786867141724,\n",
              "  0.05571301281452179,\n",
              "  0.055386897176504135,\n",
              "  0.05503907799720764,\n",
              "  0.05461374670267105,\n",
              "  0.054348886013031006,\n",
              "  0.05402778089046478,\n",
              "  0.053707629442214966,\n",
              "  0.05332321673631668,\n",
              "  0.05296427384018898,\n",
              "  0.05268629267811775,\n",
              "  0.05236925557255745,\n",
              "  0.051982637494802475,\n",
              "  0.051627419888973236,\n",
              "  0.05132277309894562,\n",
              "  0.05107923969626427,\n",
              "  0.05072256177663803,\n",
              "  0.05036549270153046,\n",
              "  0.050088003277778625,\n",
              "  0.049818046391010284,\n",
              "  0.04956206679344177,\n",
              "  0.04926621913909912,\n",
              "  0.04894312843680382,\n",
              "  0.04865984246134758,\n",
              "  0.0484146922826767,\n",
              "  0.048186663538217545,\n",
              "  0.047900743782520294,\n",
              "  0.047606129199266434,\n",
              "  0.04736030846834183,\n",
              "  0.04705492779612541,\n",
              "  0.0468817800283432,\n",
              "  0.04658496379852295,\n",
              "  0.046359773725271225,\n",
              "  0.04607440531253815,\n",
              "  0.045825645327568054,\n",
              "  0.04563145339488983,\n",
              "  0.045472756028175354,\n",
              "  0.04523301124572754,\n",
              "  0.045004211366176605,\n",
              "  0.04487781971693039,\n",
              "  0.044627174735069275,\n",
              "  0.044410474598407745,\n",
              "  0.04423604533076286,\n",
              "  0.04401247203350067,\n",
              "  0.043814126402139664,\n",
              "  0.04369429126381874,\n",
              "  0.04350520297884941,\n",
              "  0.043362442404031754,\n",
              "  0.04319451376795769],\n",
              " 'accuracy': [0.6087912321090698,\n",
              "  0.6417582631111145,\n",
              "  0.6703296899795532,\n",
              "  0.7032967209815979,\n",
              "  0.7340659499168396,\n",
              "  0.7604395747184753,\n",
              "  0.7736263871192932,\n",
              "  0.8065934181213379,\n",
              "  0.8373626470565796,\n",
              "  0.8549450635910034,\n",
              "  0.8681318759918213,\n",
              "  0.8857142925262451,\n",
              "  0.8901098966598511,\n",
              "  0.9054945111274719,\n",
              "  0.9054945111274719,\n",
              "  0.9076923131942749,\n",
              "  0.9076923131942749,\n",
              "  0.9230769276618958,\n",
              "  0.9252747297286987,\n",
              "  0.9252747297286987,\n",
              "  0.9252747297286987,\n",
              "  0.9296703338623047,\n",
              "  0.9296703338623047,\n",
              "  0.9318681359291077,\n",
              "  0.9340659379959106,\n",
              "  0.9384615421295166,\n",
              "  0.9406593441963196,\n",
              "  0.9428571462631226,\n",
              "  0.9472527503967285,\n",
              "  0.9472527503967285,\n",
              "  0.9494505524635315,\n",
              "  0.9494505524635315,\n",
              "  0.9516483545303345,\n",
              "  0.9516483545303345,\n",
              "  0.9560439586639404,\n",
              "  0.9516483545303345,\n",
              "  0.9538461565971375,\n",
              "  0.9560439586639404,\n",
              "  0.9560439586639404,\n",
              "  0.9560439586639404,\n",
              "  0.9538461565971375,\n",
              "  0.9560439586639404,\n",
              "  0.9560439586639404,\n",
              "  0.9560439586639404,\n",
              "  0.9582417607307434,\n",
              "  0.9582417607307434,\n",
              "  0.9626373648643494,\n",
              "  0.9626373648643494,\n",
              "  0.9626373648643494,\n",
              "  0.9604395627975464,\n",
              "  0.9604395627975464,\n",
              "  0.9604395627975464,\n",
              "  0.9604395627975464,\n",
              "  0.9604395627975464,\n",
              "  0.9604395627975464,\n",
              "  0.9604395627975464,\n",
              "  0.9626373648643494,\n",
              "  0.9626373648643494,\n",
              "  0.9626373648643494,\n",
              "  0.9648351669311523,\n",
              "  0.9692307710647583,\n",
              "  0.9670329689979553,\n",
              "  0.9692307710647583,\n",
              "  0.9714285731315613,\n",
              "  0.9714285731315613,\n",
              "  0.9714285731315613,\n",
              "  0.9714285731315613,\n",
              "  0.9714285731315613,\n",
              "  0.9714285731315613,\n",
              "  0.9714285731315613,\n",
              "  0.9714285731315613,\n",
              "  0.9714285731315613,\n",
              "  0.9736263751983643,\n",
              "  0.9736263751983643,\n",
              "  0.9736263751983643,\n",
              "  0.9758241772651672,\n",
              "  0.9758241772651672,\n",
              "  0.9758241772651672,\n",
              "  0.9758241772651672,\n",
              "  0.9758241772651672,\n",
              "  0.9758241772651672,\n",
              "  0.9780219793319702,\n",
              "  0.9780219793319702,\n",
              "  0.9802197813987732,\n",
              "  0.9780219793319702,\n",
              "  0.9780219793319702,\n",
              "  0.9780219793319702,\n",
              "  0.9780219793319702,\n",
              "  0.9802197813987732,\n",
              "  0.9802197813987732,\n",
              "  0.9802197813987732,\n",
              "  0.9802197813987732,\n",
              "  0.9802197813987732,\n",
              "  0.9802197813987732,\n",
              "  0.9802197813987732,\n",
              "  0.9780219793319702,\n",
              "  0.9780219793319702,\n",
              "  0.9780219793319702,\n",
              "  0.9780219793319702,\n",
              "  0.9780219793319702,\n",
              "  0.9780219793319702,\n",
              "  0.9780219793319702,\n",
              "  0.9802197813987732,\n",
              "  0.9802197813987732,\n",
              "  0.9802197813987732,\n",
              "  0.9802197813987732,\n",
              "  0.9802197813987732,\n",
              "  0.9824175834655762,\n",
              "  0.9824175834655762,\n",
              "  0.9824175834655762,\n",
              "  0.9824175834655762,\n",
              "  0.9846153855323792,\n",
              "  0.9868131875991821,\n",
              "  0.9824175834655762,\n",
              "  0.9824175834655762,\n",
              "  0.9846153855323792,\n",
              "  0.9824175834655762,\n",
              "  0.9824175834655762,\n",
              "  0.9846153855323792,\n",
              "  0.9846153855323792,\n",
              "  0.9824175834655762,\n",
              "  0.9824175834655762,\n",
              "  0.9846153855323792,\n",
              "  0.9868131875991821,\n",
              "  0.9868131875991821,\n",
              "  0.9846153855323792,\n",
              "  0.9846153855323792,\n",
              "  0.9868131875991821,\n",
              "  0.9868131875991821,\n",
              "  0.9868131875991821,\n",
              "  0.9868131875991821,\n",
              "  0.9868131875991821,\n",
              "  0.9868131875991821,\n",
              "  0.9868131875991821,\n",
              "  0.9868131875991821,\n",
              "  0.9868131875991821,\n",
              "  0.9868131875991821,\n",
              "  0.9868131875991821,\n",
              "  0.9868131875991821,\n",
              "  0.9890109896659851,\n",
              "  0.9890109896659851,\n",
              "  0.9890109896659851,\n",
              "  0.9890109896659851,\n",
              "  0.9890109896659851,\n",
              "  0.9890109896659851,\n",
              "  0.9890109896659851,\n",
              "  0.9890109896659851,\n",
              "  0.9890109896659851,\n",
              "  0.9890109896659851,\n",
              "  0.9890109896659851,\n",
              "  0.9890109896659851,\n",
              "  0.9890109896659851,\n",
              "  0.9890109896659851,\n",
              "  0.9890109896659851,\n",
              "  0.9890109896659851,\n",
              "  0.9890109896659851,\n",
              "  0.9890109896659851,\n",
              "  0.9890109896659851,\n",
              "  0.9890109896659851,\n",
              "  0.9890109896659851],\n",
              " 'val_loss': [0.6714211702346802,\n",
              "  0.6471021771430969,\n",
              "  0.62497878074646,\n",
              "  0.6047808527946472,\n",
              "  0.5850380063056946,\n",
              "  0.5662761330604553,\n",
              "  0.5481259822845459,\n",
              "  0.5305241942405701,\n",
              "  0.5126611590385437,\n",
              "  0.49574172496795654,\n",
              "  0.4799981713294983,\n",
              "  0.46456149220466614,\n",
              "  0.44897013902664185,\n",
              "  0.4348646104335785,\n",
              "  0.4213128387928009,\n",
              "  0.4076315462589264,\n",
              "  0.3936397135257721,\n",
              "  0.3787861764431,\n",
              "  0.3658549189567566,\n",
              "  0.3544972836971283,\n",
              "  0.3433745801448822,\n",
              "  0.33229902386665344,\n",
              "  0.32142260670661926,\n",
              "  0.3122539520263672,\n",
              "  0.30343499779701233,\n",
              "  0.29467517137527466,\n",
              "  0.28726813197135925,\n",
              "  0.2799360156059265,\n",
              "  0.2739902436733246,\n",
              "  0.2681349515914917,\n",
              "  0.261870414018631,\n",
              "  0.2566985785961151,\n",
              "  0.2516443431377411,\n",
              "  0.24636076390743256,\n",
              "  0.24118512868881226,\n",
              "  0.23606480658054352,\n",
              "  0.23062802851200104,\n",
              "  0.22655728459358215,\n",
              "  0.22199158370494843,\n",
              "  0.21734166145324707,\n",
              "  0.21304617822170258,\n",
              "  0.20882032811641693,\n",
              "  0.20514419674873352,\n",
              "  0.20222459733486176,\n",
              "  0.19919723272323608,\n",
              "  0.1964954137802124,\n",
              "  0.1930069774389267,\n",
              "  0.18985024094581604,\n",
              "  0.18669286370277405,\n",
              "  0.18407529592514038,\n",
              "  0.18191012740135193,\n",
              "  0.1795869767665863,\n",
              "  0.1777346283197403,\n",
              "  0.17436449229717255,\n",
              "  0.17168410122394562,\n",
              "  0.16979993879795074,\n",
              "  0.16813945770263672,\n",
              "  0.16656479239463806,\n",
              "  0.16406504809856415,\n",
              "  0.1624664068222046,\n",
              "  0.16113269329071045,\n",
              "  0.15942969918251038,\n",
              "  0.15802232921123505,\n",
              "  0.156675323843956,\n",
              "  0.15548361837863922,\n",
              "  0.15445873141288757,\n",
              "  0.1531793475151062,\n",
              "  0.1518067717552185,\n",
              "  0.15064550936222076,\n",
              "  0.14959174394607544,\n",
              "  0.14779222011566162,\n",
              "  0.14683407545089722,\n",
              "  0.1456993669271469,\n",
              "  0.14511527121067047,\n",
              "  0.14442496001720428,\n",
              "  0.1439916044473648,\n",
              "  0.14323806762695312,\n",
              "  0.14235755801200867,\n",
              "  0.14178232848644257,\n",
              "  0.1407601237297058,\n",
              "  0.14011767506599426,\n",
              "  0.13928751647472382,\n",
              "  0.13888943195343018,\n",
              "  0.13872677087783813,\n",
              "  0.13820970058441162,\n",
              "  0.137523353099823,\n",
              "  0.13709881901741028,\n",
              "  0.13644179701805115,\n",
              "  0.13614459335803986,\n",
              "  0.13556578755378723,\n",
              "  0.13495811820030212,\n",
              "  0.1345118284225464,\n",
              "  0.13356873393058777,\n",
              "  0.13260819017887115,\n",
              "  0.13226298987865448,\n",
              "  0.13211606442928314,\n",
              "  0.1318899244070053,\n",
              "  0.1314469426870346,\n",
              "  0.13100001215934753,\n",
              "  0.13074557483196259,\n",
              "  0.1304817646741867,\n",
              "  0.1302531212568283,\n",
              "  0.13007117807865143,\n",
              "  0.12978211045265198,\n",
              "  0.12956073880195618,\n",
              "  0.1292656809091568,\n",
              "  0.12917973101139069,\n",
              "  0.12895768880844116,\n",
              "  0.12876535952091217,\n",
              "  0.12854543328285217,\n",
              "  0.1285591721534729,\n",
              "  0.1282913088798523,\n",
              "  0.1274251788854599,\n",
              "  0.12723909318447113,\n",
              "  0.12710040807724,\n",
              "  0.1268470585346222,\n",
              "  0.126770481467247,\n",
              "  0.1264607459306717,\n",
              "  0.126287579536438,\n",
              "  0.12601734697818756,\n",
              "  0.12532736361026764,\n",
              "  0.12509110569953918,\n",
              "  0.12507891654968262,\n",
              "  0.12494175136089325,\n",
              "  0.12474918365478516,\n",
              "  0.1247580498456955,\n",
              "  0.12472577393054962,\n",
              "  0.12470970302820206,\n",
              "  0.12450726330280304,\n",
              "  0.12443560361862183,\n",
              "  0.12428106367588043,\n",
              "  0.1243114173412323,\n",
              "  0.12423066794872284,\n",
              "  0.12412939220666885,\n",
              "  0.12388672679662704,\n",
              "  0.12366485595703125,\n",
              "  0.12380736321210861,\n",
              "  0.12368431687355042,\n",
              "  0.12351000308990479,\n",
              "  0.12344340234994888,\n",
              "  0.1235780119895935,\n",
              "  0.12280110269784927,\n",
              "  0.12262001633644104,\n",
              "  0.12181922048330307,\n",
              "  0.12182901054620743,\n",
              "  0.12185551226139069,\n",
              "  0.12183672934770584,\n",
              "  0.12183723598718643,\n",
              "  0.12182828038930893,\n",
              "  0.12098171561956406,\n",
              "  0.12083285301923752,\n",
              "  0.12092485278844833,\n",
              "  0.12103041261434555,\n",
              "  0.12089253962039948,\n",
              "  0.12116294354200363,\n",
              "  0.12113436311483383,\n",
              "  0.12145251035690308,\n",
              "  0.12156801670789719,\n",
              "  0.1215452030301094,\n",
              "  0.12197711318731308],\n",
              " 'val_accuracy': [0.6052631735801697,\n",
              "  0.640350878238678,\n",
              "  0.6754385828971863,\n",
              "  0.6842105388641357,\n",
              "  0.7105262875556946,\n",
              "  0.7280701994895935,\n",
              "  0.7631579041481018,\n",
              "  0.7719298005104065,\n",
              "  0.780701756477356,\n",
              "  0.7894737124443054,\n",
              "  0.8070175647735596,\n",
              "  0.8157894611358643,\n",
              "  0.8245614171028137,\n",
              "  0.8245614171028137,\n",
              "  0.8508771657943726,\n",
              "  0.8684210777282715,\n",
              "  0.8947368264198303,\n",
              "  0.8947368264198303,\n",
              "  0.8947368264198303,\n",
              "  0.8947368264198303,\n",
              "  0.8947368264198303,\n",
              "  0.9035087823867798,\n",
              "  0.9035087823867798,\n",
              "  0.9122806787490845,\n",
              "  0.9122806787490845,\n",
              "  0.9035087823867798,\n",
              "  0.9035087823867798,\n",
              "  0.9122806787490845,\n",
              "  0.9122806787490845,\n",
              "  0.9122806787490845,\n",
              "  0.9122806787490845,\n",
              "  0.9122806787490845,\n",
              "  0.9210526347160339,\n",
              "  0.9210526347160339,\n",
              "  0.9210526347160339,\n",
              "  0.9210526347160339,\n",
              "  0.9210526347160339,\n",
              "  0.9210526347160339,\n",
              "  0.9298245906829834,\n",
              "  0.9298245906829834,\n",
              "  0.9385964870452881,\n",
              "  0.9385964870452881,\n",
              "  0.9385964870452881,\n",
              "  0.9385964870452881,\n",
              "  0.9385964870452881,\n",
              "  0.9473684430122375,\n",
              "  0.9473684430122375,\n",
              "  0.9473684430122375,\n",
              "  0.9473684430122375,\n",
              "  0.9473684430122375,\n",
              "  0.9473684430122375,\n",
              "  0.9473684430122375,\n",
              "  0.9473684430122375,\n",
              "  0.9561403393745422,\n",
              "  0.9561403393745422,\n",
              "  0.9561403393745422,\n",
              "  0.9561403393745422,\n",
              "  0.9561403393745422,\n",
              "  0.9561403393745422,\n",
              "  0.9561403393745422,\n",
              "  0.9561403393745422,\n",
              "  0.9561403393745422,\n",
              "  0.9561403393745422,\n",
              "  0.9561403393745422,\n",
              "  0.9561403393745422,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917,\n",
              "  0.9649122953414917]}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(pd.DataFrame(ann.history.history))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "2TF4McRlNJso",
        "outputId": "d0d5b3b9-5b21-4b44-a19c-c3264f26ff03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yp=ann.predict(xtest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hSH9bf670O0",
        "outputId": "ebd80057-c5c4-406f-b431-2cf51d50c4eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yp=np.where(yp<0.5,0,1)"
      ],
      "metadata": {
        "id": "9Qc1XL3w8u6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuw7NDgp9tRp",
        "outputId": "875ea665-458d-4952-9ecc-c5c11ec3786b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(ytest,yp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TO59ZSq9t_B",
        "outputId": "38b50bd4-6991-4994-928b-70978c02762f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.97        72\n",
            "           1       1.00      0.90      0.95        42\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.97      0.95      0.96       114\n",
            "weighted avg       0.97      0.96      0.96       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QSkXn_U792-p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}